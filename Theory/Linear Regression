== Theory of regression ==

=== Why linear regression? ===
* When my label is not just classification but the dependent variable y is
  considered continuous[1] e.g. Lets consider a supervised learning example
  where data points are houses with 2 features (X1= living area; X2 = number
  of bedrooms) and labels are the prices.

=== What is linear regression? ===
We'll take above example to explain ..
Let's say I want to predict the price (or label) Y_t from input feature X_t
Then linear regression equation would be:

    h_w(x) = w_0 + w_1*x_1 + w_2 * x_2

    where w = (w_0, w_1, w_2) are the parameters of the regression function.

Within the class of linear functions (regressors) our task shall be to find
the best parameters w. When there is no risk of confusion, we will drop w from
the h notation, and we will assume a dummy feature x0 = 1 for all datapoints
such that we can write

       D
h(x) = ∑ w_d * x_d
      d=0
where d iterates through input features 1,2,... ,D

How do I find these W parameter?
- By "best fit"

Best fit?
Well, in ML we find the model which could reduce our "error" by minimal. Of course
don't want to do overfit or underfit.

Error?
How to calculate error?
Sure, there are many ways we can calculate errors - like mean square, euclidean
distance etc

Let's stick with mean square error
J(w) =  ∑ (h_w(x_t) − y_t)2
        t

and we will naturally look for the w that minimizes the error function. The
regression obtained using the square error function J is called least square
regression, or least square fit.

There are two methods for minimizing J (Our error).

<A> Gradient Descent - Batch

The gradient descent algorithm finds a local minima of the objective function
(J) by guessing an initial set of parameters w and then ”walking” episodically
in the direction of the gradient ∂J/∂w. Since w is vector valued (3 components
for our example) we need to perform the update for each component separately

w_j = w_j − λ (∂J(w)/∂wj)

where λ is the learning rate parameter or the step of the update.
 Note that in batch gradient descent, a single update require analysis of all
 datapoints; this can be very tedious if the data set is large

Gradient descent update rule :
                      m
For all j wj = wj − λ ∑ (hw(x_i) − y_i)x_j
                     i=1
<B> Gradient Descent - Stochastic

The second way to involve all datapoints is to make the update separately for
each datapoint (stochastic gradient decent). Each update step becomes then a loop including all datapoints:

LOOP for t=1 to m
wj = wj − λ(hw(xt) − yt)xjt for all j
END LOOP

Stochastic (or online) gradient descent advances by updating based on one
datapoint at a time. if the regression problem is not too complicated, often few
iterations are enough to converge and so in practice this version of gradient
decent is often faster. it is possible (although very unlikely) to have a problem
where stochastic updates ”dance” around the best w (that realizes minimum error)
without actually converging to it.

Another thing people do is randomizing the order in which training examples are
processed to avoid having too many correlated examples in a raw which may result
in "fake" convergence.



===== Reference ====
[1] Discrete vs continuous data
Discrete variables are countable in a finite amount of time. For example, you
can count the change in your pocket. You can count the money in your bank account.
You could also count the amount of money in everyone’s bank account. It might
take you a long time to count that last item, but the point is — it’s still countable.

Continuous Variables would (literally) take forever to count. In fact, you would
get to “forever” and never finish counting them. For example, take age.
You can’t count “age”. Why not? Because it would literally take forever.
https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data

[2]
