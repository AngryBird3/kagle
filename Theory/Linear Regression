== Theory of regression ==

=== Why linear regression? ===
* When my label is not just classification but the dependent variable y is
  considered continuous[1] e.g. Lets consider a supervised learning example
  where data points are houses with 2 features (X1= living area; X2 = number
  of bedrooms) and labels are the prices.

=== What is linear regression? ===
We'll take above example to explain ..
Let's say I want to predict the price (or label) Y_t from input feature X_t
Then linear regression equation would be:

    h_w(x) = w_0 + w_1*x_1 + w_2 * x_2

    where w = (w_0, w_1, w_2) are the parameters of the regression function.

Within the class of linear functions (regressors) our task shall be to find
the best parameters w. When there is no risk of confusion, we will drop w from
the h notation, and we will assume a dummy feature x0 = 1 for all datapoints
such that we can write

       D
h(x) = ∑ w_d * x_d
      d=0
where d iterates through input features 1,2,... ,D

How do I find these W parameter?
- By "best fit"

Best fit?
Well, in ML we find the model which could reduce our "error" by minimal. Of course
don't want to do overfit or underfit.

Error?
How to calculate error?
Sure, there are many ways we can calculate errors - like mean square, euclidean
distance etc

Let's stick with mean square error
J(w) =  ∑ (h_w(x_t) − y_t)2
        t

and we will naturally look for the w that minimizes the error function. The
regression obtained using the square error function J is called least square
regression, or least square fit.

There are two methods for minimizing J (Our error).

<A> Gradient Descent

The gradient descent algorithm finds a local minima of the objective function
(J) by guessing an initial set of parameters w and then ”walking” episodically
in the direction of the gradient ∂J/∂w. Since w is vector valued (3 components
for our example) we need to perform the update for each component separately

w_j = w_j − λ (∂J(w)/∂wj)

where λ is the learning rate parameter or the step of the update.

===== Reference ====
[1] Discrete vs continuous data
Discrete variables are countable in a finite amount of time. For example, you
can count the change in your pocket. You can count the money in your bank account.
You could also count the amount of money in everyone’s bank account. It might
take you a long time to count that last item, but the point is — it’s still countable.

Continuous Variables would (literally) take forever to count. In fact, you would
get to “forever” and never finish counting them. For example, take age.
You can’t count “age”. Why not? Because it would literally take forever.
https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data

[2]
